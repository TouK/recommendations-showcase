{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf137ea4-6b82-4fab-866f-94fa8ed63f66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b141fbdc",
   "metadata": {},
   "source": [
    "# Step 1: Prepare a Shopify product inventory snapshot\n",
    "\n",
    "In this first step we prepare a snapshot of all the products that we wish to train the recommendation model on. The snapshot is a pickled python object containing the following mappings (python dictionaries):\n",
    "* `category_to_products_dict` - a mapping of a category name to a list of products in that category (i.e. `Category Name` -> `[Product Name]`)\n",
    "* `title_to_handle_dict` - a mapping of a product name to its Shopify handle (i.e. `Product Name` -> `Product Handle`)\n",
    "* `handle_to_id_dict` - a mapping of a product's Shopify handle to its Shopify GID (i.e. `Product Handle` -> `Product GID`)\n",
    "\n",
    "The purpose of each mapping will be explained later at the appropriate moments in the Notebook.\n",
    "\n",
    "The snapshot can be prepared in different ways depending on individual's needs and requirements. For this purpose, we provide three alternative options for the most common use cases. Regardless of the option you selected, at the end of this step you should have the `inventory_snapshot_path` variable pointing to the location of the pickled product inventory snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c287d3-d01c-40be-8a5e-aa84167843ff",
   "metadata": {},
   "source": [
    "## Option 1: Prepare snapshot of an existing Shopify shop with an empty inventory\n",
    "\n",
    "In this setup you have an existing Shopify shop without any products and you wish to populate the shop with example products provided by us and then train the recommendation model on those products.\n",
    "\n",
    "Start by populating the inventory of the Shopify shop with a bunch of products specified in the `./scripts/shopify/inventory.py` file using the `./scripts/shopify/populate_inventory.py` script. The script requires the following environment variables to be set:\n",
    "* `SHOPIFY_STORE_URL` - the url of the Shopify instance\n",
    "* `SHOPIFY_ACCESS_TOKEN` - the GraphQL API access token\n",
    "\n",
    "Note that the GraphQL API access token has to have the following permissions enabled: `write_product_listings`, `read_product_listings`, `write_products`, `read_products`, `write_publications`, `read_publications`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab8051b-223a-43b6-9a59-b74aec9da5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories to create (after filtering): 10\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  2.64it/s]\n",
      "Creating products for category: Clothing (ID: gid://shopify/Collection/640181698891)\n",
      "100%|███████████████████████████████████████████| 23/23 [00:15<00:00,  1.51it/s]\n",
      "Creating products for category: Electronics (ID: gid://shopify/Collection/640181731659)\n",
      "100%|███████████████████████████████████████████| 25/25 [00:16<00:00,  1.48it/s]\n",
      "Creating products for category: Books (ID: gid://shopify/Collection/640181764427)\n",
      "100%|███████████████████████████████████████████| 20/20 [00:13<00:00,  1.49it/s]\n",
      "Creating products for category: Toys (ID: gid://shopify/Collection/640181797195)\n",
      "100%|███████████████████████████████████████████| 20/20 [00:13<00:00,  1.54it/s]\n",
      "Creating products for category: Food (ID: gid://shopify/Collection/640181829963)\n",
      "100%|███████████████████████████████████████████| 27/27 [00:18<00:00,  1.49it/s]\n",
      "Creating products for category: Automotive (ID: gid://shopify/Collection/640181862731)\n",
      "100%|███████████████████████████████████████████| 21/21 [00:13<00:00,  1.51it/s]\n",
      "Creating products for category: Home (ID: gid://shopify/Collection/640181895499)\n",
      "100%|███████████████████████████████████████████| 30/30 [00:18<00:00,  1.58it/s]\n",
      "Creating products for category: Sports (ID: gid://shopify/Collection/640181928267)\n",
      "100%|███████████████████████████████████████████| 24/24 [00:16<00:00,  1.49it/s]\n",
      "Creating products for category: Beauty (ID: gid://shopify/Collection/640181961035)\n",
      "100%|███████████████████████████████████████████| 27/27 [00:17<00:00,  1.54it/s]\n",
      "Creating products for category: Health (ID: gid://shopify/Collection/640181993803)\n",
      "100%|███████████████████████████████████████████| 21/21 [00:13<00:00,  1.57it/s]\n",
      "Publishing created products to 'Online Store' channel\n",
      "100%|█████████████████████████████████████████| 238/238 [02:14<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SHOPIFY_STORE_URL\"] = \"<INSERT_SHOPIFY_STORE_URL>\"\n",
    "os.environ[\"SHOPIFY_ACCESS_TOKEN\"] = \"<INSERT_SHOPIFY_ACCESS_TOKEN>\"\n",
    "\n",
    "!python ./scripts/shopify/populate_inventory.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995f25f-7e81-41b2-8f43-f7caabe1850a",
   "metadata": {},
   "source": [
    "Note: if at any point you wish to remove the entire inventory of an existing Shopify shop and start over, use the `./scripts/shopify/cleanup.py` script. The script requires only the `SHOPIFY_STORE_URL` and `SHOPIFY_ACCESS_TOKEN` environment variables to be set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582bbed-aebf-4fe7-adb7-91939c70f1e5",
   "metadata": {},
   "source": [
    "After the shop is populated with the products, create the inventory snapshot by running the `./scripts/shopify/prepare_inventory_snapshot.py` script providing the path where to save the generated snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2afdc6-8f83-4505-bd26-6543dfa788cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopify products inventory saved in ./training/custom_inventory.pkl\n"
     ]
    }
   ],
   "source": [
    "inventory_snapshot_path = \"./training/custom_inventory.pkl\"\n",
    "\n",
    "!python ./scripts/shopify/prepare_inventory_snapshot.py $inventory_snapshot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62f106-ee57-4085-8b37-1b700b51ee3d",
   "metadata": {},
   "source": [
    "## Option 2: Prepare snapshot of an existing Shopify shop with your own products\n",
    "\n",
    "In this setup you have an existing Shopify shop with your own products and you wish to train the recommendation model on those products.\n",
    "\n",
    "Create the inventory snapshot by running the `./scripts/shopify/prepare_inventory_snapshot.py` script. Since the script requires access to the selected Shopify instance, you need to provide the following environment variables:\n",
    "* `SHOPIFY_STORE_URL` - the url of the Shopify instance\n",
    "* `SHOPIFY_ACCESS_TOKEN` - the GraphQL API access token\n",
    "\n",
    "Note that the GraphQL API access token has to have the following permissions enabled: `read_product_listings`, `read_products`, `read_publications`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1906db1-4d26-4191-8b51-7e6ed4eca469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopify products inventory saved in ./training/custom_inventory.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "inventory_snapshot_path = \"./training/custom_inventory.pkl\"\n",
    "\n",
    "os.environ[\"SHOPIFY_STORE_URL\"] = \"<INSERT_SHOPIFY_STORE_URL>\"\n",
    "os.environ[\"SHOPIFY_ACCESS_TOKEN\"] = \"<INSERT_SHOPIFY_ACCESS_TOKEN>\"\n",
    "\n",
    "!python ./scripts/shopify/prepare_inventory_snapshot.py $inventory_snapshot_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa70b77-0eb5-4a36-b61d-0f1848eb5fa1",
   "metadata": {},
   "source": [
    "## Option 3: Use the provided inventory snapshot\n",
    "\n",
    "In this setup you do not have any existing Shopify shop but you still wish to follow the notebook to experiment with recommendation model training and deployment. In this case simply use the provided inventory snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cbe264-58de-4646-a7f9-7f191fd84ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_snapshot_path = \"./training/inventory.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337536a7-2835-4868-97f3-ca229323f387",
   "metadata": {},
   "source": [
    "# Step 2: Prepare a product interactions dataset\n",
    "\n",
    "In this step, we prepare a product interactions dataset that will be used to train the chosen recommendation model. The easiest way to do this is to **generate a synthetic interactions dataset** between the products from the already generated inventory snapshot. However, if you have your own Shopify shop, you might want to **prepare a dataset based on the actual users' interactions** by e.g. dumping the Shopify clickstream (product interaction related) events obtained from the Snowplow tracker. In this case make sure that the dumped interactions dataset file has the same format as the synthetic dataset file described in this step.\n",
    "\n",
    "Before generating a synthetic interactions dataset, make sure the `inventory_snapshot_path` variable exists and points to the location of the inventory snapshot (as described in the first step). The synthetic dataset is generated with the `./scripts/ml/generate_interactions.py` script. The script uses interaction rules (between products) defined in `./scripts/ml/interaction_rules.py` and the provided products inventory snapshot to generate a reasonable simulation of users' interactions between products.\n",
    "\n",
    "Note that if you are generating the synthetic interactions dataset for your own products (you picked the 3rd option in the 1st step), then you have to modify the interaction rules specified in `./scripts/ml/interaction_rules.py` to account for product names that exist in your own inventory snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1249329-c696-4357-95d4-a00ef05ceb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session directory created in: /tmp/tmp7whyv9lc\n",
      "Total interactions generated: 73345\n",
      "Interactions dataset saved in: /tmp/tmp7whyv9lc/interactions_file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "session_dir = tempfile.mkdtemp()\n",
    "print(\"Session directory created in:\", session_dir)\n",
    "\n",
    "interactions_file = os.path.join(session_dir, \"interactions_file\")\n",
    "%run ./scripts/ml/generate_interactions.py $inventory_snapshot_path $interactions_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e55803-b057-4dbf-ae1d-458669695b66",
   "metadata": {},
   "source": [
    "Each row in the generated dataset describes a single interaction between a user and an item at a specific timestamp. Additionally we include the category of an item, which will be used as **a custom feature** to the ML model. This demonstrates the possibilities of including other custom features (such as: the color or the size of an item, the age group of a user etc) in the future.\n",
    "\n",
    "An important thing to notice is that at this point, the dataset describes the products by their Shopify handles not by their full names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59acb212-8900-45e1-b8cb-409d1d5a4789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_handle</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21993</th>\n",
       "      <td>1</td>\n",
       "      <td>user_45</td>\n",
       "      <td>dashboard-camera</td>\n",
       "      <td>1729615497</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>1</td>\n",
       "      <td>user_4</td>\n",
       "      <td>resistance-bands</td>\n",
       "      <td>1729563047</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7458</th>\n",
       "      <td>1</td>\n",
       "      <td>user_16</td>\n",
       "      <td>cardigan</td>\n",
       "      <td>1730289298</td>\n",
       "      <td>Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66563</th>\n",
       "      <td>1</td>\n",
       "      <td>user_136</td>\n",
       "      <td>compression-socks</td>\n",
       "      <td>1732043198</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30524</th>\n",
       "      <td>1</td>\n",
       "      <td>user_61</td>\n",
       "      <td>teddy-bear</td>\n",
       "      <td>1729732999</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1</td>\n",
       "      <td>user_3</td>\n",
       "      <td>slinky</td>\n",
       "      <td>1731314446</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39182</th>\n",
       "      <td>1</td>\n",
       "      <td>user_79</td>\n",
       "      <td>streaming-device</td>\n",
       "      <td>1730220362</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60635</th>\n",
       "      <td>1</td>\n",
       "      <td>user_123</td>\n",
       "      <td>car-charger</td>\n",
       "      <td>1732006687</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68216</th>\n",
       "      <td>1</td>\n",
       "      <td>user_139</td>\n",
       "      <td>hand-cream</td>\n",
       "      <td>1731578640</td>\n",
       "      <td>Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43653</th>\n",
       "      <td>1</td>\n",
       "      <td>user_89</td>\n",
       "      <td>bicycle</td>\n",
       "      <td>1731793329</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label   user_id        item_handle   timestamp item_category\n",
       "21993      1   user_45   dashboard-camera  1729615497    Automotive\n",
       "1551       1    user_4   resistance-bands  1729563047        Sports\n",
       "7458       1   user_16           cardigan  1730289298      Clothing\n",
       "66563      1  user_136  compression-socks  1732043198        Health\n",
       "30524      1   user_61         teddy-bear  1729732999          Toys\n",
       "1277       1    user_3             slinky  1731314446          Toys\n",
       "39182      1   user_79   streaming-device  1730220362   Electronics\n",
       "60635      1  user_123        car-charger  1732006687    Automotive\n",
       "68216      1  user_139         hand-cream  1731578640        Beauty\n",
       "43653      1   user_89            bicycle  1731793329        Sports"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = [\"label\", \"user_id\", \"item_handle\", \"timestamp\", \"item_category\"]\n",
    "interactions_df = pd.read_csv(interactions_file, sep='\\t', names=columns)\n",
    "interactions_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba3f04-e1d3-4bf3-a21c-b807a926d1c0",
   "metadata": {},
   "source": [
    "Note that each interaction row has an additional `label` column that is a binary label indicating whether the given interaction took place or not. This feature is present only because the data preprocessing pipeline, described in the next section, is based mostly on already existing Amazon Reviews Dataset preprocessing scripts from the [recommenders repository](https://github.com/recommenders-team/recommenders/blob/main/recommenders/datasets/amazon_reviews.py), that expect the interaction dataset in this specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e0a41-443b-4de3-882d-ab74c3e5e33c",
   "metadata": {},
   "source": [
    "# Step 3: Preprocess interactions dataset and split into train/valid/test datasets\n",
    "\n",
    "The synthetic dataset is similar to the Amazon Reviews Dataset therefore the data preprocessing pipeline consists mostly of already existing transformations implemented by [the Amazon Dataset preprocessing functions](https://github.com/recommenders-team/recommenders/blob/main/recommenders/datasets/amazon_reviews.py) in the [recommenders repository](https://github.com/recommenders-team/recommenders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9a3c50-10b3-4a1a-85df-0248087d4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.datasets.amazon_reviews import data_preprocessing, _create_vocab, _data_generating, _data_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878de562-533a-4382-b01a-cb501b118ee2",
   "metadata": {},
   "source": [
    "For each interaction in the synthetic dataset, we decide whether it will be used for training, validation or testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135a3e0c-11b0-4833-8ce9-fe4a01693c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions split file: /tmp/tmp7whyv9lc/preprocessed_output\n"
     ]
    }
   ],
   "source": [
    "assigned_interactions_file = _data_processing(interactions_file)\n",
    "print(f\"Interactions split file: {assigned_interactions_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a222a-e482-4afc-806b-c147b83b3994",
   "metadata": {},
   "source": [
    "The generated file contains the original interactions dataset with an additional `dataset` column indicating the dataset assignment of each interaction (`train`/`valid`/`test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5a99a6-bfc9-4bfe-ba40-4eaacc5ebd65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_handle</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6953</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_15</td>\n",
       "      <td>rubik-s-cube</td>\n",
       "      <td>1729546068</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6297</th>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>user_13</td>\n",
       "      <td>kite</td>\n",
       "      <td>1730187789</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4316</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_10</td>\n",
       "      <td>poetry</td>\n",
       "      <td>1731386933</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5592</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_12</td>\n",
       "      <td>coffee</td>\n",
       "      <td>1730365466</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_7</td>\n",
       "      <td>hand-cream</td>\n",
       "      <td>1731754657</td>\n",
       "      <td>Clothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_2</td>\n",
       "      <td>bluetooth-speaker</td>\n",
       "      <td>1731000358</td>\n",
       "      <td>Electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5261</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_11</td>\n",
       "      <td>fantasy</td>\n",
       "      <td>1729783923</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_11</td>\n",
       "      <td>yo-yo</td>\n",
       "      <td>1730619159</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4403</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_10</td>\n",
       "      <td>yo-yo</td>\n",
       "      <td>1729795232</td>\n",
       "      <td>Toys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7104</th>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>user_15</td>\n",
       "      <td>rice</td>\n",
       "      <td>1729589668</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  label  user_id        item_handle   timestamp item_category\n",
       "6953   train      1  user_15       rubik-s-cube  1729546068          Toys\n",
       "6297    test      1  user_13               kite  1730187789          Toys\n",
       "4316   train      1  user_10             poetry  1731386933         Books\n",
       "5592   train      1  user_12             coffee  1730365466   Electronics\n",
       "3221   train      1   user_7         hand-cream  1731754657      Clothing\n",
       "683    train      1   user_2  bluetooth-speaker  1731000358   Electronics\n",
       "5261   train      1  user_11            fantasy  1729783923         Books\n",
       "5312   train      1  user_11              yo-yo  1730619159          Toys\n",
       "4403   train      1  user_10              yo-yo  1729795232          Toys\n",
       "7104   train      1  user_15               rice  1729589668          Food"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"dataset\", \"label\", \"user_id\", \"item_handle\", \"timestamp\", \"item_category\"]\n",
    "assigned_interactions_df = pd.read_csv(assigned_interactions_file, sep='\\t', names=columns, nrows=10000)\n",
    "assigned_interactions_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18b539-f657-4020-9dc9-e891f71f2868",
   "metadata": {},
   "source": [
    "Next, we use the assignments to split the interactions dataset into training, validation and testing datasets stored in their own separate files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c30df6-52c3-42bf-a0af-c8b5c8f2a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_file = os.path.join(session_dir, 'valid_data')\n",
    "test_file = os.path.join(session_dir, 'test_data')\n",
    "train_file = os.path.join(session_dir, 'train_data')\n",
    "_data_generating(assigned_interactions_file, train_file, valid_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e509f4d-ed61-4b5a-b8aa-b98b1b7ed3ad",
   "metadata": {},
   "source": [
    "Additionally, the `_data_generating` function expands the history of individual user interactions (with products) into a time based sequence of interactions representing the user's behaviour. As an example, the following interaction history of a particular user:\n",
    "\n",
    "| item    | timestamp |\n",
    "| -------- | ------- |\n",
    "| laptop  | 1    |\n",
    "| charger | 2     |\n",
    "| mouse    | 3    |\n",
    "| keyboard | 4 |\n",
    "\n",
    "would be transformed into a sequence of interactions:\n",
    "\n",
    "| item    | timestamp |  history_item  | history_timestamp |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "| charger | 2     | laptop | 1 |\n",
    "| mouse    | 3    | laptop,charger | 1,2 |\n",
    "| keyboard | 4 |  laptop,charger,mouse | 1,2,3 |\n",
    "\n",
    "where each line contains an interaction with an item along will all the previous interactions with other items, that directly resulted in a interaction with that specific item.\n",
    "\n",
    "Below is a peak of the few expended interaction sequences for a particular user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50ff0b6a-11f0-44bb-b8a3-5cc490c8d76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_category</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>history_items</th>\n",
       "      <th>history_categories</th>\n",
       "      <th>history_timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>user_1</td>\n",
       "      <td>curtains</td>\n",
       "      <td>Home</td>\n",
       "      <td>1730172136</td>\n",
       "      <td>wardrobe</td>\n",
       "      <td>Home</td>\n",
       "      <td>1729233838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>user_1</td>\n",
       "      <td>dishwasher</td>\n",
       "      <td>Home</td>\n",
       "      <td>1730305937</td>\n",
       "      <td>wardrobe,curtains</td>\n",
       "      <td>Home,Home</td>\n",
       "      <td>1729233838,1730172136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>user_1</td>\n",
       "      <td>blender</td>\n",
       "      <td>Home</td>\n",
       "      <td>1730514206</td>\n",
       "      <td>wardrobe,curtains,dishwasher</td>\n",
       "      <td>Home,Home,Home</td>\n",
       "      <td>1729233838,1730172136,1730305937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>user_1</td>\n",
       "      <td>dining-table</td>\n",
       "      <td>Home</td>\n",
       "      <td>1730980188</td>\n",
       "      <td>wardrobe,curtains,dishwasher,blender</td>\n",
       "      <td>Home,Home,Home,Home</td>\n",
       "      <td>1729233838,1730172136,1730305937,1730514206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>user_1</td>\n",
       "      <td>blanket</td>\n",
       "      <td>Home</td>\n",
       "      <td>1729910530</td>\n",
       "      <td>wardrobe,curtains,dishwasher,blender,dining-table</td>\n",
       "      <td>Home,Home,Home,Home,Home</td>\n",
       "      <td>1729233838,1730172136,1730305937,1730514206,17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label user_id     item_name item_category   timestamp  \\\n",
       "0      1  user_1      curtains          Home  1730172136   \n",
       "1      1  user_1    dishwasher          Home  1730305937   \n",
       "2      1  user_1       blender          Home  1730514206   \n",
       "3      1  user_1  dining-table          Home  1730980188   \n",
       "4      1  user_1       blanket          Home  1729910530   \n",
       "\n",
       "                                       history_items  \\\n",
       "0                                           wardrobe   \n",
       "1                                  wardrobe,curtains   \n",
       "2                       wardrobe,curtains,dishwasher   \n",
       "3               wardrobe,curtains,dishwasher,blender   \n",
       "4  wardrobe,curtains,dishwasher,blender,dining-table   \n",
       "\n",
       "         history_categories                                 history_timestamps  \n",
       "0                      Home                                         1729233838  \n",
       "1                 Home,Home                              1729233838,1730172136  \n",
       "2            Home,Home,Home                   1729233838,1730172136,1730305937  \n",
       "3       Home,Home,Home,Home        1729233838,1730172136,1730305937,1730514206  \n",
       "4  Home,Home,Home,Home,Home  1729233838,1730172136,1730305937,1730514206,17...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"label\", \"user_id\", \"item_name\", \"item_category\", \"timestamp\", \"history_items\", \"history_categories\", \"history_timestamps\"]\n",
    "train_df = pd.read_csv(train_file, sep='\\t', names=columns, nrows=100)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e8fe0-d968-428b-bdce-89eb4fde81e4",
   "metadata": {},
   "source": [
    "Next step in dataset preprocessing is generating encodings/vocabularies (in the form of dictionaries) for every categorical features of the dataset. For this (training) dataset the categorical features are: `user_id`, `item_id` and `item_category`. Again, we use the functions provided by the Amazon Dataset processing scripts from the recommenders repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c86cc60-caf3-4473-8ddd-994255af7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vocab = os.path.join(session_dir, 'user_vocab.pkl')\n",
    "item_vocab = os.path.join(session_dir, 'item_vocab.pkl')\n",
    "cate_vocab = os.path.join(session_dir, 'category_vocab.pkl')\n",
    "_create_vocab(train_file, user_vocab, item_vocab, cate_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9babf9-a6b0-4d30-bfbb-e7b309864bea",
   "metadata": {},
   "source": [
    "The generated vocabularies can be easily explored with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34548c9b-5ec9-41fb-a6bb-09fb29cc69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "def load_vocab(filename, n=None):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        vocab = list(pkl.load(f).items())\n",
    "        return vocab if n == None else vocab[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c7df0-d6de-47b0-b4c9-6946e0ac43eb",
   "metadata": {},
   "source": [
    "that is used below to extract the first 10 encodings for each categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f6ffbf4-dbc6-4a5c-86b2-8eb988fa7e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users vocabulary (first 10 entries): [('user_107', 0), ('user_139', 1), ('user_116', 2), ('user_132', 3), ('user_58', 4), ('user_68', 5), ('user_34', 6), ('user_61', 7), ('user_43', 8), ('user_75', 9)]\n",
      "\n",
      "Items vocabulary (first 10 entries): [('default_mid', 0), ('protein-powder-1', 1), ('towel-1', 2), ('graphic-novel', 3), ('water-bottle-1', 4), ('face-mask-1', 5), ('car-charger', 6), ('jump-starter', 7), ('vitamin-c', 8), ('milk', 9)]\n",
      "\n",
      "Categories vocabulary (first 10 entries): [('default_cat', 0), ('Books', 1), ('Clothing', 2), ('Home', 3), ('Beauty', 4), ('Toys', 5), ('Food', 6), ('Automotive', 7), ('Sports', 8), ('Electronics', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Users vocabulary (first 10 entries): {load_vocab(user_vocab, n=10)}\\n\")\n",
    "print(f\"Items vocabulary (first 10 entries): {load_vocab(item_vocab, n=10)}\\n\")\n",
    "print(f\"Categories vocabulary (first 10 entries): {load_vocab(cate_vocab, n=10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4492af10-26cd-46c3-adb8-7438a1ac712e",
   "metadata": {},
   "source": [
    "For the purposes of the ML model inference and deployment, we also need to create `item handle -> category name` mappings for all the products in the interactions dataset. The deployment step of the notebook explains the purpose of this dictionary in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c5ca81-9b5c-4ef6-91b6-b366bd282c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item category mappings (first 10 entries): [('flashlight', 'Home'), ('jump-starter', 'Automotive'), ('notebook', 'Home'), ('first-aid-kit', 'Sports'), ('toaster', 'Home'), ('car-charger', 'Automotive'), ('fantasy', 'Books'), ('history', 'Books'), ('graphic-novel', 'Books'), ('science-fiction', 'Books')]\n"
     ]
    }
   ],
   "source": [
    "item_category_dict = interactions_df.set_index(\"item_handle\")[\"item_category\"].to_dict()\n",
    "item_cat_dict_file = os.path.join(session_dir, 'item_cat_dict.pkl')\n",
    "pkl.dump(item_category_dict, open(item_cat_dict_file, \"wb\"))\n",
    "\n",
    "print(f\"Item category mappings (first 10 entries): {load_vocab(item_cat_dict_file, n=10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ae9d6-e203-489f-87b1-487c75f6af54",
   "metadata": {},
   "source": [
    "The final step of data preparation is enriching validation and testing datasets with negative interaction samples, that will be used for evaluating the model during training (validation dataset) and after training (testing dataset). Specifically for every existing (thus positive) interaction in a given dataset, we generate consecutive `neg_nums_count` (negative) interactions with random items that the user has never interacted with before.\n",
    "\n",
    "The function for generating negative interactions is presented below. It was extracted from the recommenders repository and then slightly adjusted for the ease of use (the function from the recommenders repository uses constructs such as global variables, which make it harder to use it \"outside\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d2e1831-e2c8-4b59-a062-9c545932c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_negative_samples_for_file(dataset_lines, f, neg_nums_count, all_items, item_cat_dict):\n",
    "    for line in dataset_lines:\n",
    "        # write out the positive sample immediately\n",
    "        f.write(line)\n",
    "\n",
    "        # <label=1> <user_id> <item_id> <category_id> <timestamp> ...\n",
    "        words = line.strip().split(\"\\t\")\n",
    "        positive_item = words[2]\n",
    "        count = 0\n",
    "        neg_items = set()\n",
    "        while count < neg_nums_count:\n",
    "            # find a random negative item that the user has not interacted with yet\n",
    "            neg_item = random.choice(all_items)\n",
    "            if neg_item == positive_item or neg_item in neg_items:\n",
    "                continue\n",
    "\n",
    "            count += 1\n",
    "            neg_items.add(neg_item)\n",
    "\n",
    "            # append a negative interaction with a selected item\n",
    "            words[0] = \"0\"\n",
    "            words[2] = neg_item\n",
    "            words[3] = item_cat_dict[neg_item]\n",
    "            f.write(\"\\t\".join(words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51657c26-26c6-4b3b-bd5d-eba1026661da",
   "metadata": {},
   "source": [
    "Then we use the function to enrich the datasets with the negative interactions in the following way:\n",
    "* each positive interaction in the validation dataset is followed by `valid_num_ngs` negative interactions\n",
    "* each positive interaction in the test dataset is followed by `test_num_ngs` negative interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d6549ee-77e1-4579-8e9c-ae01baf36c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_num_ngs = 4 # number of negative instances with a positive instance for validation\n",
    "test_num_ngs = 9 # number of negative instances with a positive instance for testing\n",
    "\n",
    "items_list = list(interactions_df[\"item_handle\"])\n",
    "negative_sampled_test_file = os.path.join(session_dir, 'negative_sampled_test_data')\n",
    "negative_sampled_valid_file = os.path.join(session_dir, 'negative_sampled_valid_data')\n",
    "\n",
    "with open(valid_file, \"r\") as f:\n",
    "    valid_lines = f.readlines()\n",
    "\n",
    "with open(negative_sampled_valid_file, \"w\") as f:\n",
    "    _generate_negative_samples_for_file(valid_lines, f, valid_num_ngs, items_list, item_category_dict)\n",
    "\n",
    "with open(test_file, \"r\") as f:\n",
    "    test_lines = f.readlines()\n",
    "\n",
    "with open(negative_sampled_test_file, \"w\") as f:\n",
    "    _generate_negative_samples_for_file(test_lines, f, test_num_ngs, items_list, item_category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04558366-733c-4a12-b5ea-fe6ae287da52",
   "metadata": {},
   "source": [
    "At this point we have all the necessary datasets prepared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77024d81-c3e4-4e0f-897a-5894bdcfcc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset location: /tmp/tmp7whyv9lc/train_data\n",
      "Training dataset examples: 72898\n",
      "Validation dataset (sampled with 4 negative interactions) location: /tmp/tmp7whyv9lc/negative_sampled_valid_data\n",
      "Validation dataset (sampled with 4 negative interactions) examples: 745\n",
      "Testing dataset (sampled with 9 negative interactions) location: /tmp/tmp7whyv9lc/negative_sampled_test_data\n",
      "Testing dataset (sampled with 9 negative interactions) examples: 1490\n"
     ]
    }
   ],
   "source": [
    "def count_lines(filename):\n",
    "    result = !wc -l $filename | cut -d' ' -f1\n",
    "    return result[0]\n",
    "    \n",
    "print(f\"Training dataset location: {train_file}\")\n",
    "print(f\"Training dataset examples: {count_lines(train_file)}\")\n",
    "print(f\"Validation dataset (sampled with {valid_num_ngs} negative interactions) location: {negative_sampled_valid_file}\")\n",
    "print(f\"Validation dataset (sampled with {valid_num_ngs} negative interactions) examples: {count_lines(negative_sampled_valid_file)}\")\n",
    "print(f\"Testing dataset (sampled with {test_num_ngs} negative interactions) location: {negative_sampled_test_file}\")\n",
    "print(f\"Testing dataset (sampled with {test_num_ngs} negative interactions) examples: {count_lines(negative_sampled_test_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838be426-0ebe-4520-8a5b-de8af63d6f88",
   "metadata": {},
   "source": [
    "We are ready to begin training the recommendation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3cf9cc-51fa-454c-bcd9-30659cc30175",
   "metadata": {},
   "source": [
    "# Step 4: Train the SLi-Rec model\n",
    "\n",
    "The recommendation model selected for this use case is [SLi-Rec](https://www.microsoft.com/en-us/research/uploads/prod/2019/07/IJCAI19-ready_v1.pdf). The specific implementation is provided by the [recommenders repository](https://github.com/recommenders-team/recommenders/blob/main/recommenders/models/deeprec/models/sequential/sli_rec.py) and the training process mostly follows the steps outlined in the [quickstart notebook](https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/sequential_recsys_amazondataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d1ba22b-5aaa-41c8-865d-9d5d18419043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\n",
      "Tensorflow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
    "from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0123-265a-47f5-8ab4-edbbfe8f53b1",
   "metadata": {},
   "source": [
    "First we prepare the model hyperparameters.\n",
    "\n",
    "Since our training dataset contains only positive interactions (instances), we need to enable in-training negative sampling by setting the `need_sample` parameter and providing the desired number of negative instances following a positive one with the `train_num_ngs` parameter. This will enable dynamic negative sampling in mini-batches during model training. The training is done in 10 epochs with mini-batches of 400 (negative expanded) interactions, which we found to be a good enough spot for the model to converge and provide decent recommendations for our use case.\n",
    "\n",
    "The rest of the parameters are either self explanatory or were chosen as defaults from the recommenders repository quickstart notebook. Some of the parameters are provided implicitly via the `yaml_train_config_file` config file, which is mostly a copy of the original file from the recommenders repository as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575f816f-a152-4b88-a47c-a6a606cb7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 400\n",
    "yaml_train_config_file = 'training/model_train_config.yaml'\n",
    "train_num_ngs = 4 # number of negative instances with a positive instance for training\n",
    "\n",
    "hparams = prepare_hparams(yaml_train_config_file, \n",
    "                          embed_l2=0., \n",
    "                          layer_l2=0.,\n",
    "                          learning_rate=0.001,\n",
    "                          epochs=EPOCHS,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          show_step=50,\n",
    "                          MODEL_DIR=os.path.join(session_dir, \"training/model\"),\n",
    "                          SUMMARIES_DIR=os.path.join(session_dir, \"training/summary/\"),\n",
    "                          user_vocab=user_vocab,\n",
    "                          item_vocab=item_vocab,\n",
    "                          cate_vocab=cate_vocab,\n",
    "                          need_sample=True,\n",
    "                          train_num_ngs=train_num_ngs,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295af1c8-f8f6-4232-85bc-7b706f55f994",
   "metadata": {},
   "source": [
    "Next, we create a model instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8240b83-b4d0-441e-a29e-3b28c5d41cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeqModel(hparams, SequentialIterator, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2565df5-44ec-438e-9727-9e9c42714b62",
   "metadata": {},
   "source": [
    "Before we start training, we evaluate the model on the test dataset just to get a baseline of how the model is currently performing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "869d5853-8cb0-4e43-8076-2c2af4ee8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.4905, 'logloss': 0.6931, 'mean_mrr': 0.2597, 'ndcg@2': 0.1264, 'ndcg@4': 0.2138, 'ndcg@6': 0.2815, 'group_auc': 0.4974}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(negative_sampled_test_file, num_ngs=test_num_ngs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca81b26-65fe-4a85-b419-918e76490f33",
   "metadata": {},
   "source": [
    "As we can see with AUC=0.5, the untrained model behaves like random guessing whether the user would interact with a given product or not.\n",
    "\n",
    "Now, lets see if we can improve the model predictions by training it on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c913f78-dfe0-4cf0-a517-18a077ff8eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50 , total_loss: 1.6044, data_loss: 1.6044\n",
      "step 100 , total_loss: 1.5847, data_loss: 1.5847\n",
      "step 150 , total_loss: 1.5720, data_loss: 1.5720\n",
      "eval valid at epoch 1: auc:0.6382,logloss:0.6707,mean_mrr:0.5867,ndcg@2:0.5046,ndcg@4:0.6367,ndcg@6:0.6887,group_auc:0.6393\n",
      "step 50 , total_loss: 1.5576, data_loss: 1.5576\n",
      "step 100 , total_loss: 1.5604, data_loss: 1.5604\n",
      "step 150 , total_loss: 1.4980, data_loss: 1.4980\n",
      "eval valid at epoch 2: auc:0.6432,logloss:0.6434,mean_mrr:0.5824,ndcg@2:0.4997,ndcg@4:0.6573,ndcg@6:0.6859,group_auc:0.6493\n",
      "step 50 , total_loss: 1.5038, data_loss: 1.5038\n",
      "step 100 , total_loss: 1.3551, data_loss: 1.3551\n",
      "step 150 , total_loss: 1.3658, data_loss: 1.3658\n",
      "eval valid at epoch 3: auc:0.748,logloss:0.583,mean_mrr:0.6937,ndcg@2:0.6551,ndcg@4:0.7554,ndcg@6:0.771,group_auc:0.7768\n",
      "step 50 , total_loss: 1.3036, data_loss: 1.3036\n",
      "step 100 , total_loss: 1.2922, data_loss: 1.2922\n",
      "step 150 , total_loss: 1.2893, data_loss: 1.2893\n",
      "eval valid at epoch 4: auc:0.7593,logloss:0.5908,mean_mrr:0.732,ndcg@2:0.7088,ndcg@4:0.7947,ndcg@6:0.7999,group_auc:0.8138\n",
      "step 50 , total_loss: 1.2837, data_loss: 1.2837\n",
      "step 100 , total_loss: 1.2237, data_loss: 1.2237\n",
      "step 150 , total_loss: 1.2758, data_loss: 1.2758\n",
      "eval valid at epoch 5: auc:0.7686,logloss:0.6858,mean_mrr:0.767,ndcg@2:0.754,ndcg@4:0.8236,ndcg@6:0.8262,group_auc:0.8423\n",
      "step 50 , total_loss: 1.1852, data_loss: 1.1852\n",
      "step 100 , total_loss: 1.0960, data_loss: 1.0960\n",
      "step 150 , total_loss: 1.0999, data_loss: 1.0999\n",
      "eval valid at epoch 6: auc:0.7916,logloss:0.6706,mean_mrr:0.7806,ndcg@2:0.7537,ndcg@4:0.831,ndcg@6:0.8361,group_auc:0.8456\n",
      "step 50 , total_loss: 1.0899, data_loss: 1.0899\n",
      "step 100 , total_loss: 1.1338, data_loss: 1.1338\n",
      "step 150 , total_loss: 1.0950, data_loss: 1.0950\n",
      "eval valid at epoch 7: auc:0.8191,logloss:0.565,mean_mrr:0.7985,ndcg@2:0.801,ndcg@4:0.8447,ndcg@6:0.8499,group_auc:0.8658\n",
      "step 50 , total_loss: 1.1092, data_loss: 1.1092\n",
      "step 100 , total_loss: 1.1015, data_loss: 1.1015\n",
      "step 150 , total_loss: 0.9802, data_loss: 0.9802\n",
      "eval valid at epoch 8: auc:0.841,logloss:0.5473,mean_mrr:0.8179,ndcg@2:0.8278,ndcg@4:0.8619,ndcg@6:0.8645,group_auc:0.8842\n",
      "step 50 , total_loss: 0.9624, data_loss: 0.9624\n",
      "step 100 , total_loss: 0.9580, data_loss: 0.9580\n",
      "step 150 , total_loss: 0.9402, data_loss: 0.9402\n",
      "eval valid at epoch 9: auc:0.8446,logloss:0.5773,mean_mrr:0.8455,ndcg@2:0.8374,ndcg@4:0.8796,ndcg@6:0.8848,group_auc:0.896\n",
      "step 50 , total_loss: 0.9647, data_loss: 0.9647\n",
      "step 100 , total_loss: 0.9451, data_loss: 0.9451\n",
      "step 150 , total_loss: 0.9441, data_loss: 0.9441\n",
      "eval valid at epoch 10: auc:0.8365,logloss:0.6342,mean_mrr:0.8403,ndcg@2:0.8469,ndcg@4:0.8786,ndcg@6:0.8812,group_auc:0.8993\n",
      "[(1, {'auc': 0.6382, 'logloss': 0.6707, 'mean_mrr': 0.5867, 'ndcg@2': 0.5046, 'ndcg@4': 0.6367, 'ndcg@6': 0.6887, 'group_auc': 0.6393}), (2, {'auc': 0.6432, 'logloss': 0.6434, 'mean_mrr': 0.5824, 'ndcg@2': 0.4997, 'ndcg@4': 0.6573, 'ndcg@6': 0.6859, 'group_auc': 0.6493}), (3, {'auc': 0.748, 'logloss': 0.583, 'mean_mrr': 0.6937, 'ndcg@2': 0.6551, 'ndcg@4': 0.7554, 'ndcg@6': 0.771, 'group_auc': 0.7768}), (4, {'auc': 0.7593, 'logloss': 0.5908, 'mean_mrr': 0.732, 'ndcg@2': 0.7088, 'ndcg@4': 0.7947, 'ndcg@6': 0.7999, 'group_auc': 0.8138}), (5, {'auc': 0.7686, 'logloss': 0.6858, 'mean_mrr': 0.767, 'ndcg@2': 0.754, 'ndcg@4': 0.8236, 'ndcg@6': 0.8262, 'group_auc': 0.8423}), (6, {'auc': 0.7916, 'logloss': 0.6706, 'mean_mrr': 0.7806, 'ndcg@2': 0.7537, 'ndcg@4': 0.831, 'ndcg@6': 0.8361, 'group_auc': 0.8456}), (7, {'auc': 0.8191, 'logloss': 0.565, 'mean_mrr': 0.7985, 'ndcg@2': 0.801, 'ndcg@4': 0.8447, 'ndcg@6': 0.8499, 'group_auc': 0.8658}), (8, {'auc': 0.841, 'logloss': 0.5473, 'mean_mrr': 0.8179, 'ndcg@2': 0.8278, 'ndcg@4': 0.8619, 'ndcg@6': 0.8645, 'group_auc': 0.8842}), (9, {'auc': 0.8446, 'logloss': 0.5773, 'mean_mrr': 0.8455, 'ndcg@2': 0.8374, 'ndcg@4': 0.8796, 'ndcg@6': 0.8848, 'group_auc': 0.896}), (10, {'auc': 0.8365, 'logloss': 0.6342, 'mean_mrr': 0.8403, 'ndcg@2': 0.8469, 'ndcg@4': 0.8786, 'ndcg@6': 0.8812, 'group_auc': 0.8993})]\n",
      "best epoch: 10\n",
      "Time cost for training is 7.86 mins\n"
     ]
    }
   ],
   "source": [
    "with Timer() as train_time:\n",
    "    model = model.fit(train_file, negative_sampled_valid_file, valid_num_ngs=valid_num_ngs) \n",
    "\n",
    "print('Time cost for training is {0:.2f} mins'.format(train_time.interval/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce50890-0e9f-40ce-a741-19358c4f2e6d",
   "metadata": {},
   "source": [
    "The training process generated model checkpoints and summaries files inside the `hparams.SUMMARIES_DIR` directory (if it did not, then make sure the `info.write_tfevents` in `yaml_train_config_file` is set to `True`). Those summaries can be later inspected to gain a better understanding of the entire training process via the TensorBoard application:\n",
    "```shell\n",
    "$ tensorboard --logdir=<hparams.SUMMARIES_DIR>\n",
    "```\n",
    "\n",
    "After the training is done, we are ready to reevaluate our model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a9acf1f-c98c-403e-a4dd-b471ef847797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.8634, 'logloss': 0.6669, 'mean_mrr': 0.7408, 'ndcg@2': 0.7264, 'ndcg@4': 0.7941, 'ndcg@6': 0.8041, 'group_auc': 0.9165}\n"
     ]
    }
   ],
   "source": [
    "print(model.run_eval(negative_sampled_test_file, num_ngs=test_num_ngs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef34007-4260-479b-8c0c-cea6fab34792",
   "metadata": {},
   "source": [
    "Although further training might slightly improve the model's performance, the current AUC=0.86 is good enough for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc5e7b-37f2-49c9-8bd1-db79235ce7c7",
   "metadata": {},
   "source": [
    "# Step 5: Customize the trained model for deployment\n",
    "\n",
    "Before we can deploy the trained model, we need to make a few adjustments to make the model work with our use case. The best way of customizing the model logic is by creating a wrapper based on the [mlflow custom python function interface](https://mlflow.org/docs/latest/traditional-ml/creating-custom-pyfunc/index.html). This interface requires the following two functions to be provided:\n",
    "* `load_context` - invoked on every loading of the model\n",
    "* `predict` - invoked on every inference request to the model\n",
    "\n",
    "In the sections below we outline the most important customizations that the provided wrapper implements.\n",
    "\n",
    "## Model loading\n",
    "\n",
    "The process of creating an instance of a trained model involves providing the following files:\n",
    "* model weights obtained from training\n",
    "* model configuration file - we provide the configuration file for the purpose of model deployment in `./serving/model_serve_config.yaml` file. This is a stripped down version of the configuration file that is used for training\n",
    "* files generated during dataset preparation:\n",
    "  * vocabulary files\n",
    "  * the `item handle -> category name` mappings (`item_cat_dict_file`)\n",
    "  * the inventory snapshot for the purpose of mapping a product's Shopify handle to its Shopify GID\n",
    "\n",
    "The trained model instance is recreated inside the `load_context` function.\n",
    "\n",
    "## Model inference\n",
    "\n",
    "The model inference is implemented inside the `predict` function. Additional `build_batches_generator` and `encode_single_item` functions are there for the model input preprocessing.\n",
    "\n",
    "The goal of the model is to provide a list of top N (`TOP_N_HIGHEST_RECOMMENDATIONS`) best recommendations for a given user based on his/her latest sequence of interactions between products. Since the selected model is trained as a binary classifier, it only calculates a probability of a positive interaction with a single given product. Therefore to construct of list of top N best recommendations, it is necassary to infer the model on all the products in the inventory - that is why the model wrapper has to have an access to the products inventory. Additionally to speed up the model inference on the entire products inventory, the products are grouped and inferred in `BATCH_SIZE`ed batches (`build_batches_generator` function). Once the interaction probabilities are inferred on the entire inventory, the list is sorted and the `TOP_N_HIGHEST_RECOMMENDATIONS` products with the highest positive interaction probabilities are selected. As an additional step, the selected products are mapped to their respective Shopify GIDs which are then returned by the model. Note that this Shopify GID mapping is there mostly as a convenient way to avoid doing it inside the Nussknacker scenario itself. \n",
    "\n",
    "## Custom item and user features\n",
    "\n",
    "The model allows the incorporation of custom item and user features, however this is not provided out of the box and requires changes in the model implementation. Features are represented as additional embedding layers of specified sizes that get merged together and placed at the \"beginning\" of the actual model graph. The process is clearly illustrated in `_build_embedding` and `_lookup_from_embedding` methods of the [SequentialBaseModel](https://github.com/recommenders-team/recommenders/blob/main/recommenders/models/deeprec/models/sequential/sequential_base_model.py) class.\n",
    "\n",
    "As usual, categorial features require some form of encoding before they are passed to the model. In this example we use vocabularies, generated during model training, based on the known variants (present in the training dataset) of each feature. This is the way we encode the user identifier (`user_vocab` artifact), product Shopify handle (`item_vocab` artifact) and category name (`category_vocab` artifact). The vocabularies are uploaded together with other model artifacts and are used during model inference for encoding provided values of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28328db9-24ad-45de-b44a-1dec993008cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "artifacts = {\n",
    "    \"model_data\" : os.path.join(session_dir, \"training/model/\"),\n",
    "    \"model_config\": \"serving/model_serve_config.yaml\",\n",
    "    \"user_vocab\" : user_vocab,\n",
    "    \"item_vocab\" : item_vocab,\n",
    "    \"category_vocab\" : cate_vocab,\n",
    "    \"item_category_dict\": item_cat_dict_file,\n",
    "    \"inventory_snapshot\": inventory_snapshot_path,\n",
    "}\n",
    "\n",
    "class SliRecModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        from recommenders.models.deeprec.models.sequential.sli_rec import SLI_RECModel as SeqModel\n",
    "        from recommenders.models.deeprec.io.sequential_iterator import SequentialIterator\n",
    "        from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "        import numpy as np\n",
    "\n",
    "        hparams = prepare_hparams(\n",
    "            context.artifacts[\"model_config\"],\n",
    "            user_vocab=context.artifacts[\"user_vocab\"],\n",
    "            item_vocab=context.artifacts[\"item_vocab\"],\n",
    "            cate_vocab=context.artifacts[\"category_vocab\"],\n",
    "        )\n",
    "        \n",
    "        self.model = SeqModel(hparams, SequentialIterator)\n",
    "        self.model.load_model(context.artifacts[\"model_data\"] + \"./artifacts/best_model\")\n",
    "        self.item_cat_dict = self.load_dict(context.artifacts[\"item_category_dict\"])\n",
    "        self.item_handle_to_id_dict = self.load_dict(context.artifacts[\"inventory_snapshot\"])[\"handle_to_id_dict\"]\n",
    "        self.TOP_N_HIGHEST_RECOMMENDATIONS = 16\n",
    "        self.BATCH_SIZE = 16\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        import numpy as np\n",
    "        import time\n",
    "        \n",
    "        user_id = model_input['userId'][0]\n",
    "        history_item_ids = model_input['items'][0]\n",
    "        history_timestamps = model_input['timestamps'][0]\n",
    "        now_timestamp = int(time.time())\n",
    "\n",
    "        batches = self.build_batches_generator(user_id, now_timestamp, history_item_ids, history_timestamps)\n",
    "\n",
    "        inferred_items = []\n",
    "        inferred_preds = []\n",
    "        for (batch, items) in batches:\n",
    "            preds = self.model.infer(self.model.sess, batch)\n",
    "            inferred_preds += (preds[0].flatten().tolist())\n",
    "            inferred_items += items\n",
    "\n",
    "        top_n_item_indices = np.argsort(inferred_preds)[::-1][:self.TOP_N_HIGHEST_RECOMMENDATIONS]\n",
    "        return [{\n",
    "            \"itemIds\": list(map(lambda h: self.item_handle_to_id_dict[h], np.array(inferred_items)[top_n_item_indices].tolist())),\n",
    "            \"preds\": np.array(inferred_preds)[top_n_item_indices].tolist()\n",
    "        }]\n",
    "\n",
    "    def build_batches_generator(self, user_id, now_timestamp, history_item_ids, history_timestamps, min_seq_length=1):\n",
    "        it = self.model.iterator\n",
    "        history_item_categories = [self.item_cat_dict[k] for k in history_item_ids]\n",
    "        \n",
    "        batched_item_ids = []\n",
    "        label_list = []\n",
    "        user_list = []\n",
    "        item_list = []\n",
    "        item_cate_list = []\n",
    "        item_history_batch = []\n",
    "        item_cate_history_batch = []\n",
    "        time_list = []\n",
    "        time_diff_list = []\n",
    "        time_from_first_action_list = []\n",
    "        time_to_now_list = []\n",
    "\n",
    "        cnt = 0\n",
    "        for item_id, item_category in self.item_cat_dict.items():\n",
    "            encoded_item = self.encode_single_item(\n",
    "                user_id,\n",
    "                item_id,\n",
    "                item_category,\n",
    "                now_timestamp,\n",
    "                history_item_ids,\n",
    "                history_item_categories,\n",
    "                history_timestamps\n",
    "            )\n",
    "\n",
    "            if len(encoded_item[\"historyItemIds\"]) < min_seq_length:\n",
    "                continue\n",
    "\n",
    "            batched_item_ids.append(item_id)\n",
    "            user_list.append(encoded_item[\"userId\"])\n",
    "            item_list.append(encoded_item[\"itemId\"])\n",
    "            item_cate_list.append(encoded_item[\"itemCategory\"])\n",
    "            item_history_batch.append(encoded_item[\"historyItemIds\"])\n",
    "            item_cate_history_batch.append(encoded_item[\"historyCategories\"])\n",
    "            time_list.append(encoded_item[\"currentTime\"])\n",
    "            time_diff_list.append(encoded_item[\"timeDiff\"])\n",
    "            time_from_first_action_list.append(encoded_item[\"timeFromFirstAction\"])\n",
    "            time_to_now_list.append(encoded_item[\"timeToNow\"])\n",
    "\n",
    "            # label is useless for prediction but required for SliRec conversion utilities\n",
    "            label_list.append(0)\n",
    "\n",
    "            cnt += 1\n",
    "            if cnt == self.BATCH_SIZE:\n",
    "                res = it._convert_data(\n",
    "                    label_list,\n",
    "                    user_list,\n",
    "                    item_list,\n",
    "                    item_cate_list,\n",
    "                    item_history_batch,\n",
    "                    item_cate_history_batch,\n",
    "                    time_list,\n",
    "                    time_diff_list,\n",
    "                    time_from_first_action_list,\n",
    "                    time_to_now_list,\n",
    "                    0,\n",
    "                )\n",
    "                batch_feed_dict = it.gen_feed_dict(res)\n",
    "                yield (batch_feed_dict, batched_item_ids) if batch_feed_dict else None\n",
    "                \n",
    "                batched_item_ids = []\n",
    "                label_list = []\n",
    "                user_list = []\n",
    "                item_list = []\n",
    "                item_cate_list = []\n",
    "                item_history_batch = []\n",
    "                item_cate_history_batch = []\n",
    "                time_list = []\n",
    "                time_diff_list = []\n",
    "                time_from_first_action_list = []\n",
    "                time_to_now_list = []\n",
    "                cnt = 0\n",
    "        # process the remaining inputs in the last batch\n",
    "        if cnt > 0:\n",
    "            res = it._convert_data(\n",
    "                label_list,\n",
    "                user_list,\n",
    "                item_list,\n",
    "                item_cate_list,\n",
    "                item_history_batch,\n",
    "                item_cate_history_batch,\n",
    "                time_list,\n",
    "                time_diff_list,\n",
    "                time_from_first_action_list,\n",
    "                time_to_now_list,\n",
    "                0,\n",
    "            )\n",
    "            batch_feed_dict = it.gen_feed_dict(res)\n",
    "            yield (batch_feed_dict, batched_item_ids) if batch_feed_dict else None\n",
    "\n",
    "    # extracted and adjusted based on the SequentialIterator from the recommenders module\n",
    "    # https://github.com/recommenders-team/recommenders/blob/main/recommenders/models/deeprec/io/sequential_iterator.py\n",
    "    def encode_single_item(self, userId, itemId, itemCategory, nowTimestamp, historyItemIds, historyCategories, historyTimestamps):\n",
    "        import numpy as np\n",
    "        \n",
    "        it = self.model.iterator\n",
    "        \n",
    "        user_id = it.userdict[userId] if userId in it.userdict else 0\n",
    "        item_id = it.itemdict[itemId] if itemId in it.itemdict else 0\n",
    "        item_cate = it.catedict[itemCategory] if itemCategory in it.catedict else 0\n",
    "        current_time = float(nowTimestamp)\n",
    "\n",
    "        item_history_sequence = []\n",
    "        cate_history_sequence = []\n",
    "        time_history_sequence = []\n",
    "    \n",
    "        for item in historyItemIds:\n",
    "            item_history_sequence.append(\n",
    "                it.itemdict[item] if item in it.itemdict else 0\n",
    "            )\n",
    "        \n",
    "        for cate in historyCategories:\n",
    "            cate_history_sequence.append(\n",
    "                it.catedict[cate] if cate in it.catedict else 0\n",
    "            )\n",
    "\n",
    "        time_history_sequence = [float(i) for i in historyTimestamps]\n",
    "        time_range = 3600 * 24\n",
    "\n",
    "        time_diff = []\n",
    "        for i in range(len(time_history_sequence) - 1):\n",
    "            diff = (time_history_sequence[i + 1] - time_history_sequence[i]) / time_range\n",
    "            diff = max(diff, 0.5)\n",
    "            time_diff.append(diff)\n",
    "    \n",
    "        last_diff = (current_time - time_history_sequence[-1]) / time_range\n",
    "        last_diff = max(last_diff, 0.5)\n",
    "        time_diff.append(last_diff)\n",
    "        time_diff = np.log(time_diff)\n",
    "\n",
    "        time_from_first_action = []\n",
    "        first_time = time_history_sequence[0]\n",
    "        time_from_first_action = [\n",
    "            (t - first_time) / time_range for t in time_history_sequence[1:]\n",
    "        ]\n",
    "        time_from_first_action = [max(t, 0.5) for t in time_from_first_action]\n",
    "        last_diff = (current_time - first_time) / time_range\n",
    "        last_diff = max(last_diff, 0.5)\n",
    "        time_from_first_action.append(last_diff)\n",
    "        time_from_first_action = np.log(time_from_first_action)\n",
    "\n",
    "        time_to_now = []\n",
    "        time_to_now = [(current_time - t) / time_range for t in time_history_sequence]\n",
    "        time_to_now = [max(t, 0.5) for t in time_to_now]\n",
    "        time_to_now = np.log(time_to_now)\n",
    "\n",
    "        return {\n",
    "            \"userId\": user_id,\n",
    "            \"itemId\": item_id,\n",
    "            \"itemCategory\": item_cate,\n",
    "            \"historyItemIds\": item_history_sequence,\n",
    "            \"historyCategories\": cate_history_sequence,\n",
    "            \"currentTime\": current_time,\n",
    "            \"timeDiff\": time_diff,\n",
    "            \"timeFromFirstAction\": time_from_first_action,\n",
    "            \"timeToNow\": time_to_now\n",
    "        }\n",
    "\n",
    "    def load_dict(self, filename):\n",
    "        import pickle as pkl\n",
    "        \n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d500e5-cc1d-4bf6-96a8-07e601a2bc62",
   "metadata": {},
   "source": [
    "# Step 6: Log the model to Databricks registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96841403-51b4-4faf-b0b0-cf8b1040c7aa",
   "metadata": {},
   "source": [
    "The last step is to deploy the model wrapper to the Databricks registry. When executing the notebook inside the Databricks environment, the deployment process is trivial and comes down to invoking the `mlflow.pyfunc.log_model` function that accepts:\n",
    "* the name of the model - this is the name that will be used later to refer to the model via the MLFlow enricher\n",
    "* model artifacts - those are all the files needed to recreate an instance of the model (explained in more detail in step 5)\n",
    "* model signature - the example based signature for the model\n",
    "* conda environment - hints about the required dependencies needed by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5839db-04b4-452c-a84f-59b6121703e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mst/anaconda3/envs/test_rec_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 3955.02it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2187.95it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4245.25it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2468.69it/s]\n",
      "Downloading artifacts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2686.93it/s]\n",
      "Uploading artifacts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.58it/s]\n",
      "Registered model 'slirec_shopify_model' already exists. Creating a new version of this model...\n",
      "2024/11/15 17:25:31 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: slirec_shopify_model, version 2\n",
      "Created version '2' of model 'slirec_shopify_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "\n",
    "signature = infer_signature(model_input = {\n",
    "    \"userId\": \"<not-important-user-id>\",\n",
    "    \"items\": [\"thermometer\", \"comic-book\", \"brush\"],\n",
    "    \"timestamps\": [\"1730807061\", \"1730808119\", \"1730809203\"]\n",
    "}, model_output = {\n",
    "    \"itemIds\": [\"gid://Product/14809004540249\", \"gid://Product/14809003753817\", \"gid://Product/14809013879129\"],\n",
    "    \"preds\": [0.9762271046638489, 0.8966923356056213, 0.6756445169448853]\n",
    "})\n",
    "\n",
    "default_conda_env = mlflow.pyfunc.get_default_conda_env()\n",
    "default_conda_env['dependencies'].append('tensorflow=2.12.1')\n",
    "default_conda_env['dependencies'].append('recommenders=1.2.0')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"slirec_shopify_model\",\n",
    "        python_model=SliRecModelWrapper(),\n",
    "        conda_env=default_conda_env,\n",
    "        artifacts=artifacts,\n",
    "        registered_model_name=\"slirec_shopify_model\",\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a41aef-5907-44fa-90ed-531a142f29b6",
   "metadata": {},
   "source": [
    "The last line of the deployment log shows the name of the deployed model along with the specific version that was assigned to the model. Knowing this information allows us to select the appropriate model in the MLFlow enricher. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
